{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370db347",
   "metadata": {},
   "source": [
    "# Load Sequential Mover Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905b8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83227\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'data/2018_10/space_time_mover_feature.json'\n",
    "\n",
    "with open(file_name, \"r\", encoding='UTF-8') as read_file:\n",
    "    mover_id_seqs_dict = json.load(read_file)\n",
    "\n",
    "print(len(mover_id_seqs_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32f3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "34\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for key, val in mover_id_seqs_dict.items():\n",
    "    print(len(val)) # Number of Trips for a mover\n",
    "    print(len(val[0])) # Number of feature dimensions for a trip\n",
    "    print(val[0][0]) # A feature value\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4957c",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc1c030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA Availability\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d12d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mover_count = len(mover_id_seqs_dict)\n",
    "max_seq_len = -1\n",
    "\n",
    "tup_list = []\n",
    "for mover_id, val in mover_id_seqs_dict.items():\n",
    "    curr_seq_len = len(val)\n",
    "    if max_seq_len < curr_seq_len:\n",
    "        max_seq_len = curr_seq_len\n",
    "    tup = (mover_id, curr_seq_len)\n",
    "    tup_list.append(tup)\n",
    "\n",
    "tup_list.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e775bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tup_list[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa78368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch import optim\n",
    "from importlib import reload\n",
    "import seq2seq as ss\n",
    "reload(ss)\n",
    "\n",
    "input_size = 34\n",
    "output_size = 34\n",
    "hidden_size = 256\n",
    "mini_batch = 32\n",
    "learning_rate=0.001\n",
    "\n",
    "encoder = ss.EncoderMSE(input_size, hidden_size).to(device)\n",
    "decoder = ss.DecoderMSE(input_size, hidden_size, output_size).to(device)\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6485fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n"
     ]
    }
   ],
   "source": [
    "packed_inputs = []\n",
    "packed_inputs_d = []\n",
    "batch_sizes = []\n",
    "seq_sums = []\n",
    "\n",
    "batch_count = math.ceil(mover_count / mini_batch)\n",
    "for i in range(batch_count):\n",
    "    batch_list = []\n",
    "    seq_len = []\n",
    "    seq_sum = 0\n",
    "    for j in range(mini_batch):\n",
    "        curr_idx = i * mini_batch + j\n",
    "        if curr_idx < mover_count:\n",
    "            m_id = tup_list[curr_idx][0]\n",
    "            m_seq = mover_id_seqs_dict[m_id]\n",
    "            m_tensor = torch.tensor(m_seq, dtype=torch.float, device=device)\n",
    "            batch_list.append(m_tensor)\n",
    "            seq_len.append(len(m_seq))\n",
    "            seq_sum += len(m_seq)\n",
    "            # print(m_tensor.shape)\n",
    "    # print(seq_len)\n",
    "    padded = pad_sequence(batch_list, batch_first=True)\n",
    "    packed = pack_padded_sequence(padded, seq_len, batch_first=True)\n",
    "    packed_inputs.append(packed)\n",
    "    batch_sizes.append(len(seq_len))\n",
    "    \n",
    "    d_batch_list = []\n",
    "    for t in batch_list:\n",
    "        a = torch.zeros(1, input_size, device=device)\n",
    "        b = t[:-1, :]\n",
    "        c = torch.cat((a, b), 0)\n",
    "        d_batch_list.append(c)\n",
    "    padded = pad_sequence(d_batch_list, batch_first=True)\n",
    "    packed = pack_padded_sequence(padded, seq_len, batch_first=True)\n",
    "    packed_inputs_d.append(packed)\n",
    "    seq_sums.append(seq_sum)\n",
    "    \n",
    "print(len(packed_inputs))\n",
    "print(len(batch_sizes))\n",
    "print(len(packed_inputs_d))\n",
    "print(len(seq_sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3d987d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0 / loss: 94.29853756539524\n",
      "ep: 1 / loss: 53.90490900259465\n",
      "ep: 2 / loss: 44.68914312822744\n",
      "ep: 3 / loss: 39.5065615803469\n",
      "ep: 4 / loss: 35.89267413970083\n",
      "ep: 5 / loss: 33.77257702499628\n",
      "ep: 6 / loss: 31.643703725654632\n",
      "ep: 7 / loss: 30.03594262432307\n",
      "ep: 8 / loss: 28.630087205208838\n",
      "ep: 9 / loss: 27.42208548914641\n",
      "ep: 10 / loss: 26.421024458832107\n",
      "ep: 11 / loss: 25.544815358589403\n",
      "ep: 12 / loss: 24.397146325209178\n",
      "ep: 13 / loss: 23.574240879737772\n",
      "ep: 14 / loss: 22.917159031610936\n",
      "ep: 15 / loss: 22.2184208733961\n",
      "ep: 16 / loss: 21.47734271886293\n",
      "ep: 17 / loss: 21.001590630272403\n",
      "ep: 18 / loss: 20.54298361047404\n",
      "ep: 19 / loss: 20.23264828941319\n",
      "ep: 20 / loss: 19.60151431831764\n",
      "ep: 21 / loss: 19.16377099917736\n",
      "ep: 22 / loss: 19.157031289010774\n",
      "ep: 23 / loss: 18.336504135397263\n",
      "ep: 24 / loss: 18.171917235886212\n",
      "ep: 25 / loss: 17.786542741116136\n",
      "ep: 26 / loss: 17.622201663441956\n",
      "ep: 27 / loss: 17.404232125147246\n",
      "ep: 28 / loss: 16.9472063651192\n",
      "ep: 29 / loss: 16.99385205109138\n",
      "ep: 30 / loss: 16.50084558117669\n",
      "ep: 31 / loss: 16.467172956326976\n",
      "ep: 32 / loss: 16.046552860934753\n",
      "ep: 33 / loss: 16.19913834950421\n",
      "ep: 34 / loss: 15.620361082721502\n",
      "ep: 35 / loss: 15.40847483446123\n",
      "ep: 36 / loss: 15.44528207636904\n",
      "ep: 37 / loss: 15.291740847576875\n",
      "ep: 38 / loss: 14.775822151219472\n",
      "ep: 39 / loss: 15.107760292245075\n",
      "ep: 40 / loss: 15.05644971539732\n",
      "ep: 41 / loss: 14.706123196519911\n",
      "ep: 42 / loss: 14.531125719600823\n",
      "ep: 43 / loss: 14.52988870779518\n",
      "ep: 44 / loss: 14.567014643689618\n",
      "ep: 45 / loss: 14.34047898609424\n",
      "ep: 46 / loss: 14.163464955519885\n",
      "ep: 47 / loss: 14.082402333966456\n",
      "ep: 48 / loss: 13.846385191136505\n",
      "ep: 49 / loss: 13.969533803581726\n",
      "Training Time : 376.688s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "losses = []\n",
    "epoch = 50\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Minibatch iteration\n",
    "for ep in range(epoch):\n",
    "    \n",
    "    indexes = list(range(len(packed_inputs)))\n",
    "    random.shuffle(indexes)\n",
    "\n",
    "    counter = 0\n",
    "    ep_loss = 0\n",
    "    for idx in indexes:\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        encoder_input = packed_inputs[idx]\n",
    "        batch_size = batch_sizes[idx]\n",
    "        encoder_h0 = torch.zeros(1, batch_size, hidden_size, device=device)\n",
    "\n",
    "        _, h_n = encoder(encoder_input, encoder_h0)\n",
    "\n",
    "        decoder_input = packed_inputs_d[idx]\n",
    "        decoder_h0 = h_n\n",
    "\n",
    "        decoder_output, _ = decoder(decoder_input, decoder_h0)\n",
    "\n",
    "        unpacked, unpacked_len = pad_packed_sequence(encoder_input, batch_first=True)\n",
    "\n",
    "        if decoder_output.shape != unpacked.shape:\n",
    "            print('Error')\n",
    "            break\n",
    "\n",
    "        loss = criterion(decoder_output, unpacked)\n",
    "\n",
    "#         if counter % 200 == 0:\n",
    "#             loss_val = loss.item() / seq_sums[idx] * 1000\n",
    "#             print(counter, loss_val)\n",
    "#             losses.append(loss_val)\n",
    "        ep_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        # counter += 1\n",
    "    print('ep: {} / loss: {}'.format(ep, ep_loss))\n",
    "    losses.append(ep_loss)\n",
    "    \n",
    "elapsed_time = round(time.time() - start_time, 3)\n",
    "print('Training Time : {}s'.format(elapsed_time))\n",
    "    \n",
    "# print('Final Avg :', sum(losses) / len(losses))\n",
    "# ss.show_plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb03c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_h0 = torch.zeros(1, 1, hidden_size, device=device)\n",
    "encoder.eval()\n",
    "\n",
    "m_ids = []\n",
    "features = []\n",
    "mover_embed_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tup in tup_list:\n",
    "        m_id = tup[0]\n",
    "        m_seq = mover_id_seqs_dict[m_id]\n",
    "        m_tensor = torch.tensor(m_seq, dtype=torch.float, device=device)\n",
    "        padded = pad_sequence([m_tensor], batch_first=True)\n",
    "        encoder_input = pack_padded_sequence(padded, [len(m_seq)], batch_first=True)\n",
    "        _, h_n = encoder(encoder_input, encoder_h0)\n",
    "        h_n = h_n.view(-1)\n",
    "        h_n_np = h_n.cpu().numpy()\n",
    "        \n",
    "        m_ids.append(m_id)\n",
    "        features.append(h_n_np)\n",
    "        mover_embed_dict[m_id] = {\n",
    "            'latent': h_n_np.tolist(),\n",
    "            'umap': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f1d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81256\n",
      "277596 256\n"
     ]
    }
   ],
   "source": [
    "print(len(mover_embed_dict))\n",
    "for key, val in mover_embed_dict.items():\n",
    "    print(key, len(val['latent']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f0bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "def run_and_write_umap(nn):\n",
    "    file_name = 'data/2018_01_2018_05/space_time_mover_umap_ep_50_nn_{}.json'.format(nn)\n",
    "    print('nn: {} Start'.format(nn))\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=nn,\n",
    "        n_epochs= 500\n",
    "    )\n",
    "    embedding = reducer.fit_transform(features)\n",
    "    \n",
    "    for idx, m_id in enumerate(m_ids):\n",
    "        pt = embedding[idx]\n",
    "        mover_embed_dict[m_id]['umap'] = pt.tolist()\n",
    "        \n",
    "    new_dict = {}\n",
    "    for key, val in mover_embed_dict.items():\n",
    "        if 'latent' in val:\n",
    "            del val['latent']\n",
    "        new_dict[key] = val['umap']\n",
    "        \n",
    "    with open(file_name, \"w\", encoding='UTF-8') as write_file:\n",
    "        json.dump(new_dict, write_file, separators=(',', ':'), indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print('nn: {} Done'.format(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae99f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn: 2 Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n",
      "C:\\Users\\Shin\\Anaconda3\\envs\\moverva\\lib\\site-packages\\umap\\spectral.py:256: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn: 2 Done\n",
      "nn: 5 Start\n",
      "nn: 5 Done\n",
      "nn: 10 Start\n",
      "nn: 10 Done\n",
      "nn: 20 Start\n",
      "nn: 20 Done\n",
      "nn: 50 Start\n",
      "nn: 50 Done\n",
      "nn: 100 Start\n",
      "nn: 100 Done\n"
     ]
    }
   ],
   "source": [
    "nn_list = [2, 5, 10, 20, 50, 100]\n",
    "\n",
    "for nn in nn_list:\n",
    "    run_and_write_umap(nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4b821",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2a093fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83227\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=40,\n",
    "    n_epochs= 500\n",
    ")\n",
    "\n",
    "embedding = reducer.fit_transform(features)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b8a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, m_id in enumerate(m_ids):\n",
    "    pt = embedding[idx]\n",
    "    mover_embed_dict[m_id]['umap'] = pt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496cdad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83227\n",
      "277596\n",
      "[2.3555071353912354, -2.366137981414795]\n"
     ]
    }
   ],
   "source": [
    "print(len(mover_embed_dict))\n",
    "for key, val in mover_embed_dict.items():\n",
    "    print(key)\n",
    "    print(val['umap'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d817b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'data/space_time/space_time_mover_latent_umap_ep_50.json'\n",
    "\n",
    "with open(file_name, \"w\", encoding='UTF-8') as write_file:\n",
    "    json.dump(mover_embed_dict, write_file, separators=(',', ':'), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a13a190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83227\n",
      "277596\n",
      "{'umap': [2.3555071353912354, -2.366137981414795]}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key, val in mover_embed_dict.items():\n",
    "    del val['latent']\n",
    "    new_dict[key] = val\n",
    "    \n",
    "print(len(new_dict))\n",
    "for key, val in new_dict.items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0602ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'data/space_time/space_time_mover_umap_ep_50.json'\n",
    "\n",
    "with open(file_name, \"w\", encoding='UTF-8') as write_file:\n",
    "    json.dump(new_dict, write_file, separators=(',', ':'), indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
